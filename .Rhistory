library(vegan)
data(iris)
iris
data(iris)
pca = rda(iris[, 1:4])
plot(pca)
biplot(pca)
pca = princomp(iris[, 1:4])
biplot(pca)
data(iris)
pca = princomp(iris[, 1:4])
biplot(pca)
summary(pca)
str(pca)
loadings(pca)
str(biplot(pca))
str(pca)
scores(pca)
pca$loadings
loadings(pca)
pca$scores
princomp
pca = prcomp(iris[, 1:4])
biplot(pca)
str(pca)
pca$loadings
pca$scores
summary(pca)
library(vegan)
data(iris)
rda(~iris[, 1:4])
rda(iris[, 1:4])
pca = rda(iris[, 1:4])
plot(pca)
plot(pca, scaleing=1)
biplot(pca, scaleing=1)
biplot(pca, scaling=1)
biplot(pca, scaling=2)
data(varechem)
data(varespec)
library(vegan)
data(varechem)
data(varespec)
# SOLVE
X = scale(as.matrix(varechem), center=TRUE, scale=FALSE)
X[1:3, 1:3]
Y = scale(as.matrix(varespec))
Y[1:4, 1:4]
coef_solve = solve(t(X) %*% X) %*% t(X) %*% Y
coef_solve[, 1:2]
Yhat_solve = X %*% coef_solve
Yhat_solve[1:4, 1:4]
# LM
n = nrow(Y)
Y_hat = Y
Y_res = Y
coef_lm = matrix(ncol=ncol(X), nrow = ncol(Y))
lin_model = list()
for (i in 1:ncol(Y)) {
df_i = data.frame(species=Y[, i], X)
lin_model[[i]] = lm(species ~ 0+., df_i)
coef_lm[i, ] = coef(lin_model[[i]])
Y_hat[, i] = predict(lin_model[[i]])
Y_res[, i] = residuals(lin_model[[i]])
}
# trace is the total variance of Y
trace = sum(diag(cov(Y)))
trace_res = sum(diag(cov(Y_res)))
# 3) compute PCA on Yhat and Yres
svd_ = svd(Y_hat)
u_ = svd_$u
s_ = svd_$d
vt_ = svd_$v
svd_res = svd(Y_res)
u_res = svd_res$u
s_res = svd_res$d
vt_res = svd_res$v
## precompute eigenvalues to compute kc and kc_res
eigenvalues = s_**2/(n-1)
eigenvalues_res = s_res**2/(n-1)
kc = length(which(eigenvalues > 0.00000001))
kc_res = length(which(eigenvalues_res > 0.00000001))
## eigenvalues and eigenvectors considering rank kc
eigenvalues = eigenvalues[1:kc]
eigenvalues_res = eigenvalues[1:kc_res]
eigenvectors = vt_[,1:kc]
eigenvectors_res = vt_res[,1:kc_res]
singular_values_all = c(s_[1:kc], s_res[1:kc])
# 4) Ordination of objects (site scores, or vegan's wa scores)
F_ = Y %*% eigenvectors # columns of F are the ordination vectors
F_res = Y_res %*% eigenvectors_res # columns of F are the ordination vectors
# 5) F in space X (site constraints, or vegan's lc scores)
Z = Y_hat %*% eigenvectors
Z_res = Y_res %*% eigenvectors_res
# 6) Correlation between the ordination vectors in spaces Y and X
rk = cor(F_, Z) # not used yet
rk_res = cor(F_res, Z_res) # not used yet
# 7) Contribution of the explanatory variables X to the canonical ordination
# axes
# 7.1) C (canonical coefficient): the weights of the explanatory variables X in
# the formation of the matrix of fitted site scores
C = B.dot(eigenvectors)  # not used yet
# 7) Contribution of the explanatory variables X to the canonical ordination
# axes
# 7.1) C (canonical coefficient): the weights of the explanatory variables X in
# the formation of the matrix of fitted site scores
C = B %*% eigenvectors  # not used yet
B = matrix(ncol=ncol(X), nrow = ncol(Y))
lin_model = list()
for (i in 1:ncol(Y)) {
df_i = data.frame(species=Y[, i], X)
lin_model[[i]] = lm(species ~ 0+., df_i)
B[i, ] = coef(lin_model[[i]])
Y_hat[, i] = predict(lin_model[[i]])
Y_res[, i] = residuals(lin_model[[i]])
}
# 7) Contribution of the explanatory variables X to the canonical ordination
# axes
# 7.1) C (canonical coefficient): the weights of the explanatory variables X in
# the formation of the matrix of fitted site scores
C = B %*% eigenvectors  # not used yet
# LM
n = nrow(Y)
Y_hat = Y
Y_res = Y
B = matrix(ncol=ncol(X), nrow = ncol(Y))
lin_model = list()
for (i in 1:ncol(Y)) {
df_i = data.frame(species=Y[, i], X)
lin_model[[i]] = lm(species ~ 0+., df_i)
B[i, ] = coef(lin_model[[i]])
Y_hat[, i] = predict(lin_model[[i]])
Y_res[, i] = residuals(lin_model[[i]])
}
# trace is the total variance of Y
trace = sum(diag(cov(Y)))
trace_res = sum(diag(cov(Y_res)))
# 3) compute PCA on Yhat and Yres
svd_ = svd(Y_hat)
u_ = svd_$u
s_ = svd_$d
vt_ = svd_$v
svd_res = svd(Y_res)
u_res = svd_res$u
s_res = svd_res$d
vt_res = svd_res$v
## precompute eigenvalues to compute kc and kc_res
eigenvalues = s_**2/(n-1)
eigenvalues_res = s_res**2/(n-1)
kc = length(which(eigenvalues > 0.00000001))
kc_res = length(which(eigenvalues_res > 0.00000001))
## eigenvalues and eigenvectors considering rank kc
eigenvalues = eigenvalues[1:kc]
eigenvalues_res = eigenvalues[1:kc_res]
eigenvectors = vt_[,1:kc]
eigenvectors_res = vt_res[,1:kc_res]
singular_values_all = c(s_[1:kc], s_res[1:kc])
B
eigenvectors
# 7) Contribution of the explanatory variables X to the canonical ordination
# axes
# 7.1) C (canonical coefficient): the weights of the explanatory variables X in
# the formation of the matrix of fitted site scores
C = B %*% eigenvectors  # not used yet
B
B = matrix(ncol=ncol(Y), nrow = ncol(X))
lin_model = list()
for (i in 1:ncol(Y)) {
df_i = data.frame(species=Y[, i], X)
lin_model[[i]] = lm(species ~ 0+., df_i)
B[, i] = coef(lin_model[[i]])
Y_hat[, i] = predict(lin_model[[i]])
Y_res[, i] = residuals(lin_model[[i]])
}
C = B %*% eigenvectors  # not used yet
C_res = B %*% eigenvectors_res  # not used yet
# 7.2) The correlations between X and the ordination vectors in space X are
# used to represent the explanatory variables in biplots.
#corXZ = corr(x.as_matrix().T, Z.T)
#corXZ_res = corr(x.as_matrix().T, Z_res.T)
corXZ = corr(X, Z)
corXZ_res = corr(X, Z_res)
# 7.2) The correlations between X and the ordination vectors in space X are
# used to represent the explanatory variables in biplots.
#corXZ = corr(x.as_matrix().T, Z.T)
#corXZ_res = corr(x.as_matrix().T, Z_res.T)
corXZ = cor(X, Z)
corXZ_res = cor(X, Z_res)
# LM
scaling = 1
## scaling factors
if (scaling == 1) {
scaling_factor = const
D = diag(np.sqrt(eigenvalues/trace)) # Diagonal matrix of weights (Numerical Ecology with R, p. 196)
D_res = diag(np.sqrt(eigenvalues_res/trace_res))
} else if (scaling == 2) {
scaling_factor = singular_values_all / const
D = np.diag(np.ones(kc)) # Diagonal matrix of weights
D_res = np.diag(np.ones(kc_res))
}
scaling_factor = constante
## scaling factors
const = np.sum(singular_values_all**2)**0.25
## scaling factors
const = sum(singular_values_all**2)**0.25
if (scaling == 1) {
scaling_factor = const
D = diag(np.sqrt(eigenvalues/trace)) # Diagonal matrix of weights (Numerical Ecology with R, p. 196)
D_res = diag(np.sqrt(eigenvalues_res/trace_res))
} else if (scaling == 2) {
scaling_factor = singular_values_all / const
D = np.diag(np.ones(kc)) # Diagonal matrix of weights
D_res = np.diag(np.ones(kc_res))
}
D = diag(rep(1, kc)) # Diagonal matrix of weights
D
if (scaling == 1) {
scaling_factor = const
D = diag(sqrt(eigenvalues/trace)) # Diagonal matrix of weights (Numerical Ecology with R, p. 196)
D_res = diag(sqrt(eigenvalues_res/trace_res))
} else if (scaling == 2) {
scaling_factor = singular_values_all / const
D = diag(rep(1, kc)) # Diagonal matrix of weights
D_res = diag(rep(1, kc_res))
}
response_scores = data.frame(cbind((eigenvectors, eigenvectors_res)) * scaling_factor,
row.names=response_ids,
colnames=ordi_column_names_all)
response_scores = data.frame(cbind((eigenvectors, eigenvectors_res)) * scaling_factor,
row.names=response_ids,
colnames=ordi_column_names_all)
response_scores = data.frame(cbind(eigenvectors, eigenvectors_res) * scaling_factor,
row.names=response_ids,
colnames=ordi_column_names_all)
feature_ids = colnames(X)
sample_ids = rownames(X) # x index and y index should be the same
response_ids = colnames(Y)
response_scores = data.frame(cbind(eigenvectors, eigenvectors_res) * scaling_factor,
row.names=response_ids,
colnames=ordi_column_names_all)
## axes names
ordi_column_names = paste0('RDA', 1:kc) # ['RDA%d' % (i+1) for i in range(kc)]
ordi_column_names_res =paste0('RDA_res', 1:kc_res)
response_scores = data.frame(cbind(eigenvectors, eigenvectors_res) * scaling_factor,
row.names=response_ids,
colnames=ordi_column_names_all)
ordi_column_names_all = c(ordi_column_names, ordi_column_names_res)
response_scores = data.frame(cbind(eigenvectors, eigenvectors_res) * scaling_factor,
row.names=response_ids,
colnames=ordi_column_names_all)
eigenvectors
eigenvectors_res
response_ids
ordi_column_names_all
library(vegan)
data(varechem)
data(varespec)
# SOLVE
X = scale(as.matrix(varechem), center=TRUE, scale=FALSE)
X[1:3, 1:3]
Y = scale(as.matrix(varespec))
Y[1:4, 1:4]
coef_solve = solve(t(X) %*% X) %*% t(X) %*% Y
coef_solve[, 1:2]
Yhat_solve = X %*% coef_solve
Yhat_solve[1:4, 1:4]
# LM
scaling = 1
feature_ids = colnames(X)
sample_ids = rownames(X) # x index and y index should be the same
response_ids = colnames(Y)
n = nrow(Y)
Y_hat = Y
Y_res = Y
B = matrix(ncol=ncol(Y), nrow = ncol(X))
lin_model = list()
for (i in 1:ncol(Y)) {
df_i = data.frame(species=Y[, i], X)
lin_model[[i]] = lm(species ~ 0+., df_i)
B[, i] = coef(lin_model[[i]])
Y_hat[, i] = predict(lin_model[[i]])
Y_res[, i] = residuals(lin_model[[i]])
}
# trace is the total variance of Y
trace = sum(diag(cov(Y)))
trace_res = sum(diag(cov(Y_res)))
# 3) compute PCA on Yhat and Yres
svd_ = svd(Y_hat)
u_ = svd_$u
s_ = svd_$d
vt_ = svd_$v
svd_res = svd(Y_res)
u_res = svd_res$u
s_res = svd_res$d
vt_res = svd_res$v
## precompute eigenvalues to compute kc and kc_res
eigenvalues = s_**2/(n-1)
eigenvalues_res = s_res**2/(n-1)
kc = length(which(eigenvalues > 0.00000001))
kc_res = length(which(eigenvalues_res > 0.00000001))
## eigenvalues and eigenvectors considering rank kc
eigenvalues = eigenvalues[1:kc]
eigenvalues_res = eigenvalues[1:kc_res]
eigenvectors = vt_[,1:kc]
eigenvectors_res = vt_res[,1:kc_res]
singular_values_all = c(s_[1:kc], s_res[1:kc])
## axes names
ordi_column_names = paste0('RDA', 1:kc) # ['RDA%d' % (i+1) for i in range(kc)]
ordi_column_names_res =paste0('RDA_res', 1:kc_res)
ordi_column_names_all = c(ordi_column_names, ordi_column_names_res)
# 4) Ordination of objects (site scores, or vegan's wa scores)
F_ = Y %*% eigenvectors # columns of F are the ordination vectors
F_res = Y_res %*% eigenvectors_res # columns of F are the ordination vectors
# 5) F in space X (site constraints, or vegan's lc scores)
Z = Y_hat %*% eigenvectors
Z_res = Y_res %*% eigenvectors_res
# 6) Correlation between the ordination vectors in spaces Y and X
rk = cor(F_, Z) # not used yet
rk_res = cor(F_res, Z_res) # not used yet
# 7) Contribution of the explanatory variables X to the canonical ordination
# axes
# 7.1) C (canonical coefficient): the weights of the explanatory variables X in
# the formation of the matrix of fitted site scores
C = B %*% eigenvectors  # not used yet
C_res = B %*% eigenvectors_res  # not used yet
# 7.2) The correlations between X and the ordination vectors in space X are
# used to represent the explanatory variables in biplots.
#corXZ = corr(x.as_matrix().T, Z.T)
#corXZ_res = corr(x.as_matrix().T, Z_res.T)
corXZ = cor(X, Z)
corXZ_res = cor(X, Z_res)
## scaling factors
const = sum(singular_values_all**2)**0.25
if (scaling == 1) {
scaling_factor = const
D = diag(sqrt(eigenvalues/trace)) # Diagonal matrix of weights (Numerical Ecology with R, p. 196)
D_res = diag(sqrt(eigenvalues_res/trace_res))
} else if (scaling == 2) {
scaling_factor = singular_values_all / const
D = diag(rep(1, kc)) # Diagonal matrix of weights
D_res = diag(rep(1, kc_res))
}
eigenvectors
dim(eigenvectors)
dim(eigenvectors_res)
response_scores = data.frame(cbind(eigenvectors, eigenvectors_res) * scaling_factor,
row.names=response_ids,
colnames=ordi_column_names_all)
response_scores = data.frame(cbind(eigenvectors, eigenvectors_res) * scaling_factor)
dim(cbind(eigenvectors, eigenvectors_res) * scaling_factor)
length(ordi_column_names_all)
length(response_ids)
response_scores = data.frame(cbind(eigenvectors, eigenvectors_res) * scaling_factor,
row.names=response_ids,
col.names=ordi_column_names_all)
colnames(response_scores) = ordi_column_names_all
rownames(response_scores) = response_ids
response_scores = data.frame(cbind(eigenvectors, eigenvectors_res) * scaling_factor)
colnames(response_scores) = ordi_column_names_all
rownames(response_scores) = response_ids
# response scores (species)
response_scores = data.frame(eigenvectors, eigenvectors_res) * scaling_factor
colnames(sample_scores) = ordi_column_names_all
rownames(sample_scores) = sample_ids
sample_scores_type = 'wa'
if (sample_scores_type == 'wa') {
sample_scores = data.frame(F_, F_res) / scaling_factor
} else if (sample_scores_type == 'lc') {
sample_scores = data.frame(Z, Z_res) / scaling_factor
}
colnames(sample_scores) = ordi_column_names_all
rownames(sample_scores) = sample_ids
sample_scores
# biplot scores (environmental constrains)
biplot_scores = data.frame(corXZ %*% D, corXZ_ers %*% D_res) * scaling_factor
colnames(biplot_scores) = ordi_column_names_all
# biplot scores (environmental constrains)
biplot_scores = data.frame(corXZ %*% D, corXZ_res %*% D_res) * scaling_factor
colnames(biplot_scores) = ordi_column_names_all
rownames(biplot_scores) = feature_ids
#
sample_constraints = data.frame(Z, F_res) / scaling_factor
colnames(sample_constraints) = ordi_column_names_all
rownames(sample_constraints) = sample_ids
# Goodness of fit
## Unadjusted R2
R2 = np.sum(eigenvalues/trace)
# Goodness of fit
## Unadjusted R2
R2 = sum(eigenvalues/trace)
## Adjusted R2
R2a = 1-((n-1)/(n-m-1))*(1-R2)
n = nrow(Y)
m = ncol(X)
## Adjusted R2
R2a = 1-((n-1)/(n-m-1))*(1-R2)
p_explained = singular_values_all / sum(singular_values_all)
p_explained
View(eigenvectors_res)
doubs_species = read.csv('data/DoubsSpe.csv')
setwd = '/home/essicolo/Documents/GitHub/nuee'
doubs_species = read.csv('data/DoubsSpe.csv')
doubs_env = pd.read_csv('data/DoubsEnv.csv', index_col=0)
getwd()
setwd('/home/essicolo/Documents/GitHub/nuee')
doubs_species = read.csv('data/DoubsSpe.csv')
doubs_env = pd.read_csv('data/DoubsEnv.csv', index_col=0)
doubs_env = read.csv('data/DoubsEnv.csv')
doubs_species = read.csv('data/DoubsSpe.csv')
doubs_env = read.csv('data/DoubsEnv.csv')
Y = apply(doubs_species, 2, function(x) x/mean(x))
Y
doubs_env
W = doubs_env[, c('das', 'alt', 'pen', 'deb')]
X = doubs_env[, c('pH', 'dur', 'pho', 'nit', 'amm', 'oxy', 'dbo')]
head(W)
head(X)
